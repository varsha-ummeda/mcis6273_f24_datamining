{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad84fa9-62d8-4171-b51d-32e3d64a5ab0",
   "metadata": {},
   "source": [
    "# unzip the given zip (data) file\n",
    "# Loading and extracting the data.csv and inspecting the first 5 rows using head method\n",
    "Load the extracted CSV into a pandas DataFrame and inspect the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eadc966-25d8-4e0d-997c-0434cbd9cedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Column_0     Column_1         Column_2                 Column_3    Column_4  \\\n",
      "0      NaN          key  doi-asserted-by                      DOI  first-page   \n",
      "1        0  e_1_2_7_2_1        publisher         10.1038/35041545         NaN   \n",
      "2        1  e_1_2_7_3_1        publisher  10.1126/science.1155121         NaN   \n",
      "3        2  e_1_2_7_4_1        publisher    10.1007/s003820050007         NaN   \n",
      "4        3  e_1_2_7_5_1        publisher     10.1029/2000GL012471         NaN   \n",
      "\n",
      "       Column_5 Column_6 Column_7 Column_8       Column_9  ... Column_20  \\\n",
      "0  volume-title   author     year   volume  journal-title  ...       NaN   \n",
      "1           NaN      NaN      NaN      NaN            NaN  ...       NaN   \n",
      "2           NaN      NaN      NaN      NaN            NaN  ...       NaN   \n",
      "3           NaN      NaN      NaN      NaN            NaN  ...       NaN   \n",
      "4           NaN      NaN      NaN      NaN            NaN  ...       NaN   \n",
      "\n",
      "  Column_21 Column_22 Column_23 Column_24 Column_25 Column_26 Column_27  \\\n",
      "0       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "1       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "2       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "3       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "4       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "\n",
      "  Column_28 Column_29  \n",
      "0       NaN       NaN  \n",
      "1       NaN       NaN  \n",
      "2       NaN       NaN  \n",
      "3       NaN       NaN  \n",
      "4       NaN       NaN  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Dataset has been successfully imported into the DataFrame.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 24\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset has been successfully imported into the DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Data cleaning: ensure the second column is treated as a string for further processing\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Convert the second column to a string type for string-based operations\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m data[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Locate rows where the second column contains the substring 'key', ignoring case sensitivity\u001b[39;00m\n\u001b[0;32m     27\u001b[0m key_rows \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m, case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, na\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Ensure the target directory exists or create it if needed\n",
    "os.makedirs('./data/', exist_ok=True)\n",
    "\n",
    "# Unzip the dataset into the specified directory\n",
    "with zipfile.ZipFile('hw0_dataset_1M.csv.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./data/')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the dataset from the CSV file, ensuring bad lines are skipped and optimized for large files\n",
    "file_path = \"./data/hw0_dataset_1M.csv\"\n",
    "data = pd.read_csv(file_path, header=None, names=[f\"Column_{i}\" for i in range(30)], on_bad_lines='skip', dtype=str, low_memory=True)\n",
    "\n",
    "# Display the first few rows to preview the structure of the dataset\n",
    "print(data.head())\n",
    "print(\"Dataset has been successfully imported into the DataFrame.\")\n",
    "\n",
    "# Data cleaning: ensure the second column is treated as a string for further processing\n",
    "\n",
    "# Convert the second column to a string type for string-based operations\n",
    "data[1] = data[1].astype(str)\n",
    "\n",
    "# Locate rows where the second column contains the substring 'key', ignoring case sensitivity\n",
    "key_rows = data[data[1].str.contains('key', case=False, na=False)]\n",
    "\n",
    "# Show the first few rows that contain the word 'key'\n",
    "print(key_rows.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4afdb9c-f2b4-4564-92fa-d23d8e9de5cb",
   "metadata": {},
   "source": [
    "# 1.Converting the second column to string type and search for rows which contains the 'key'\n",
    "# 2.Saving the cleaned data to cleaned_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8529dd-97e2-4953-9d37-f9b7e7c1f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the second column is of type string for uniform processing\n",
    "df[1] = df[1].astype(str)\n",
    "\n",
    "# Identify rows in column 1 that contain the word 'key', case insensitive\n",
    "key_matching_rows = df[df[1].str.contains('key', case=False, na=False)]\n",
    "\n",
    "# Identify the row with the most non-NaN values, which will be used as the new header\n",
    "max_non_null_row = key_matching_rows.loc[key_matching_rows.notna().sum(axis=1).idxmax()]\n",
    "\n",
    "# Set this row as the new header for the DataFrame\n",
    "df.columns = max_non_null_row\n",
    "\n",
    "# Remove the rows used for finding the header, as they are no longer needed\n",
    "df = df.drop(key_matching_rows.index)\n",
    "\n",
    "# Remove columns with NaN values in the header and drop rows that consist entirely of NaNs\n",
    "cleaned_df = df.loc[:, df.columns.notna()]\n",
    "cleaned_df = cleaned_df.dropna(how='all')\n",
    "\n",
    "# Display the first few rows of the cleaned data\n",
    "print(cleaned_df.head())\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV file and print confirmation\n",
    "df_cleaned.to_csv('data/cleaned_data.csv', index=False)\n",
    "print(\"The cleaned data has been saved.\")\n",
    "\n",
    "# Save the selected columns to another CSV file with a new name\n",
    "output_csv_path = 'data/cleaned_dataset_selected_columns.csv'\n",
    "cleaned_df.to_csv(output_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8570d45c-35a7-49c6-9906-90e8208fabf3",
   "metadata": {},
   "source": [
    "# 1.Extract,transform and export JSON data\n",
    "# 2.Restrict the data to the required 3 columns: 'DOI', 'journal-title', and 'doi-asserted-by'\n",
    "# 3. Normalizing \n",
    "# 4. Saving to data_cleaned_normalized csv\n",
    "# 5.Saving to data_cleaned_normalized json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3cefde-1ff6-48a3-99dd-79eaf89785b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df_transformed = df_cleaned[['DOI', 'journal-title', 'doi-asserted-by']].copy()\n",
    "\n",
    "\n",
    "df_transformed['DOI'] = df_transformed['DOI'].str.lower()\n",
    "df_transformed['journal-title'] = df_transformed['journal-title'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()) if pd.notnull(x) else x)\n",
    "\n",
    "df_transformed['journal-title'] = df_transformed['journal-title'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x) if pd.notnull(x) else x)\n",
    "\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "#saving to csv\n",
    "csv_filename = 'data/data_cleaned_normalized.csv'\n",
    "df_transformed.to_csv(csv_filename, index=False)\n",
    "#saving to json\n",
    "json_filename = 'data/data_cleaned_normalized.json'\n",
    "df_transformed.to_json(json_filename, orient='records', lines=True)\n",
    "print(\"Succesful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea94258-4308-4cd9-82bc-dc628783cc81",
   "metadata": {},
   "source": [
    "# 1.Filter, transform and export CSV data.\n",
    "# 2. getting journal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750004d-0905-425f-9d7b-0bd201ca4e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def filter_and_export_data(df, csv_output_path, json_output_path):\n",
    "    df_filtered = df.dropna(subset=['DOI', 'journal-title'])\n",
    "    df_filtered_unique = df_filtered.drop_duplicates(subset='DOI')\n",
    "    print(\"Columns before selection:\", df_filtered_unique.columns)\n",
    "    df_filtered_final = df_filtered_unique[['DOI', 'journal-title']].copy()  \n",
    "    print(\"Columns after selection:\", df_filtered_final.columns)\n",
    "    os.makedirs(os.path.dirname(csv_output_path), exist_ok=True)\n",
    "    df_filtered_final.to_csv(csv_output_path, index=False)\n",
    "    df_filtered_final.to_json(json_output_path, orient='records', lines=True)\n",
    "    print(f\"Filtered data saved to '{csv_output_path}' and '{json_output_path}'.\")\n",
    "\n",
    "csv_output_path = 'data/data_filtered.csv'\n",
    "json_output_path = 'data/data_filtered.json'\n",
    "\n",
    "filter_and_export_data(df_cleaned, csv_output_path, json_output_path)\n",
    "\n",
    "def n_most_frequent_doi(df, n):\n",
    "    doi_counts = df['DOI'].value_counts().head(n)\n",
    "    return doi_counts.reset_index().rename(columns={'index': 'DOI', 'DOI': 'count'})\n",
    "\n",
    "def journal_lookup(df, s):\n",
    "    filtered_df = df[df['journal-title'].str.contains(s, case=False, na=False)]\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Get the 5 most frequent DOIs\n",
    "most_frequent_dois = n_most_frequent_doi(df_cleaned, 5)\n",
    "print(most_frequent_dois)\n",
    "\n",
    "# Lookup journals with 'Science' in the title\n",
    "journal_results = journal_lookup(df_cleaned, 'Science')\n",
    "print(journal_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0517694-cc06-46cb-8153-e5569357d826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6567aa-d317-4cb2-8334-78ea4742436b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
